<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pytorch | Alexandre Jouandin</title>
    <link>/tags/pytorch/</link>
      <atom:link href="/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <description>pytorch</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019 Alexandre Jouandin</copyright><lastBuildDate>Wed, 08 May 2019 18:23:53 -0400</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>pytorch</title>
      <link>/tags/pytorch/</link>
    </image>
    
    <item>
      <title>Transformer Model</title>
      <link>/project/transformer-model/</link>
      <pubDate>Wed, 08 May 2019 18:23:53 -0400</pubDate>
      <guid>/project/transformer-model/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;We propose an in-depth analysis and reimplementation of the Transformer model (Vaswani et al., NIPS 2017). Its non-recurrent behavior and sole use of attention makes it an intriguing model to analyze. We perform a hyper-parameters search, as well as a memory-profiling study, both of these allowing us to successfully train and semantically evaluate the model on the IWSLT TED Translation task.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://deepfrench.gitlab.io/deep-learning-project&#34;&gt;Read more on the project&#39;s website&amp;hellip;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
